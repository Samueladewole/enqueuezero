---
title: 2018.50
---

# 2018.50

## Stack Overflow: How We Do Monitoring - 2018 Edition

[Read original post](https://nickcraver.com/blog/2018/11/29/stack-overflow-how-we-do-monitoring/)

The operation engineers in Stack Overflow care three major things:

* Logs
* Health Checks
* Metrics

They use tools [Bosun](https://github.com/bosun-monitor/), Grafana, PagerDuty, Pingdom, Opserve.r, MiniProfiler, etc.

The pain point is it's not clear which service is the root of cause when an incident is ongoing, considering the system has many services and dependencies. Another potential improvement is to gain more from reading metrics.

## Deep learning: the final frontier for signal processing and time series analysis?

[Read original post](https://medium.com/@alexrachnog/deep-learning-the-final-frontier-for-signal-processing-and-time-series-analysis-734307167ad6)

* Systems like Cosmic, economy, and human creates time series data.
* The classic approaches for processing time series data includes:
    * Time domain analysis
    * Frequency domain analysis
    * Nearest neighbors analysis
    * (S)AR(I)MA(X) models
    * Decomposition
    * Nonlinear dynamics
    * Machine Learning
* The deep learning approaches for processing time series data includes:
    * RNN
    * CNN
    * Autoregressive CNN
    * Clustering
    * Anomaly detection
    * Hybrid
* Conclusion: Autoregressive CNN > CNN > RNN. If possible, combine DL with mathematical modeling.

## GitHub Project: [shubheksha/kubernetes-internals](https://github.com/shubheksha/kubernetes-internals)

This is a collection of resources that shed light on the inner workings of Kubernetes

Instead of just starring the project, I quickly scanned through some documents. I'll present more simplified notes later.

* [What happens when k8s](https://github.com/jamiehannaford/what-happens-when-k8s): The long post explains what happens when typing `kubectl`. It involves below steps:
    * Kubectl loads `.kubeconfig` and its context.
    * Kubectl generates the token in HTTP headers.
    * Kube-apiserver validates the token (authentication).
    * Kube-apiserver validates if the owner of the token can perform the action (authorization, RBAC).
    * Kube-apiserver validates if the certain action can be applied to the node (admission controllers)
    * Etcd save payloads.
    * Initializers performs some logics before the resource is visible to the apiserver or scheduler. (optional, based on if the resource type has initializers)
    * Controllers like deployments controller, replicasets controller setup resources.
    * Scheduler schedules resources to certain nodes or pods.
    * Kubelet in worker node syncs resources regularly.
    * Kubelet instantiates resources, for example, CRI starting containers, CNI assigning IPs and setting up network bridges, overlay networking updating inter-host networking rules.
    * Container is up.
* [Kubernetes 101 Networking](http://www.dasblinkenlichten.com/kubernetes-101-networking/): This post explains the network flow between pods. In particular, the networking in this post refers to the physical network that connects hosts together. It's a nice post demonstrating the network flow based on utilities like tcpdump, iptables, etc.
    * All containers in a pod shares the same IP. The diagram is like below:
        * the rest of the network -> node eth0
        * node eth0 -> docker0
        * docker0 -> veth0
        * veth0 -> Pod1container1
        * veth0 -> Pod1container2
        * docker0 -> vethN
        * vethN -> Pod2container1
        * ...
    * Kubernetes `Service` provides load balancing mechanism across pods hosting the same service. The diagram is like below:
        * iptables on each host have rules for redirecting traffic from serviceIp:servicePort to localhost:hostPort.
        * kubernetes-proxy decides which host should load balance to.
* [Understanding kubernetes networking: pods](https://medium.com/google-cloud/understanding-kubernetes-networking-pods-7117dd28727).
    *  A pod consists of one or more containers that are collocated on the same host, and are configured to share a network stack and other resources such as volumes.
    * "Share a network stack" means all the containers in a pod can reach each other on localhost.
    * The connection between a container and the bridge is established over a pair of linked virtual ethernet devices, one in the container network namespace and the other in the root network namespace.
    * Kubernetes creates a special container for each pod whose only purpose is to provide a network interface for the other containers. It's implemented by a single `pause` command sleeping until Kubernetes sending SIGTERM.
    * The `veth0` interface is created with the `pause` container and is visible to all other containers in the same pod.
    * Kubernetes uses `cbr0` (custom bridge) instead of `docker0`.

---

## GitHub Project: [kinghajj/deque](https://github.com/kinghajj/deque)

A (mostly) lock-free concurrent work-stealing deque in Rust.

This module implements the Chase-Lev work stealing deque described in "Dynamic Circular Work-Stealing Deque".

Usage:

```rust
use deque;

let (worker, stealer) = deque::new();

// Only the worker may push/pop
worker.push(1);
worker.pop();

// Stealers take data from the other end of the deque
worker.push(1);
stealer.steal();

// Stealers can be cloned to have many stealers stealing in parallel
worker.push(1);
let stealer2 = stealer.clone();
stealer2.steal();
```

The `Deque` itself is a struct tracking `bottom`, `top` and an `array`. Note that all these values are Atomic values, meaning increasing or decreasing is guaranteed to behave correctly by the CPU:

```rust
struct Deque<T: Send> {
    bottom: AtomicIsize,
    top: AtomicIsize,
    array: AtomicPtr<Buffer<T>>,
}
```

Both the worker and stealer have a reference to the deque.

* Worker can only access to one side of the deque by using the `push` and `pop` operation.
* Stealers can only access to the other side of the deque by using the `steal` operation.

```rust
pub struct Worker<T: Send> {
    deque: Arc<Deque<T>>,
    marker: PhantomData<Cell<()>>,
}

pub struct Stealer<T: Send> {
    deque: Arc<Deque<T>>,
}
```

Note that the field `marker` ensures the access to the worker is from a single thread at once.

When stealing data, there are three possible outcomes:

* The deque was empty at the time of stealing.
* The stealer lost the race for stealing data.
* The stealer has successfully stolen some data.

```rust
pub enum Stolen<T> { Empty, Abort, Data(T), }
```

An internal circular buffer is used as the intermediate storage of the data in the deque.

```rust
struct Buffer<T: Send> {
    storage: *mut T,
    size: usize,
    prev: Option<Box<Buffer<T>>>,
}
```

The implementation of `push` has below instructions:

* Grow the buffer if it is full.
* Put the data into the array, and increase the bottom value.

The implementation of `pop` has below instructions:

* Fail fast if the deque is empty.
* Decrease the bottom value.
* Get the data from the array.
* Return the data if no race, otherwise, return nothing.

The implementation of `steal` has below instructions:

* Get the data from the array.
* Return the data if no race, otherwise, return nothing.

Conclusions:

* The implementation is heavily based on the implementation using C11
  atomics in "Correct and Efficient Work Stealing for Weak Memory Models".
* The only potentially lock-synchronized portion of this deque is the
  occasional call to the memory allocator when growing the deque. Otherwise
  all operations are lock-free.

---


