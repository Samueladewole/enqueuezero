---
title: EnqueueZero Techshack 2019-02
---

# EnqueueZero Techshack 2019-02

<TechshackHeader />

## Dive into Deep Learning

[d2l.ai](https://d2l.ai/index.html) | [zh.d2l.ai](https://zh.d2l.ai/)

This is a nice book to keep up the tread of Deep Learning. It explains the [Deep Learning Basics](https://d2l.ai/chapter_deep-learning-basics/index.html) [CNN](https://d2l.ai/chapter_convolutional-neural-networks/index.html), [RNN](https://d2l.ai/chapter_recurrent-neural-networks/index.html), [NLP](https://d2l.ai/chapter_natural-language-processing/index.html), etc. Each chapter goes with mathematics, figures and code.

## The Architecture of Skipper

[opensource.zalando.com](https://opensource.zalando.com/skipper/reference/architecture/)

[Skipper](https://opensource.zalando.com/skipper/reference/architecture/) is an HTTP router and reverse proxy for service composition.

* It's released as a server package `skipper` and a client package `eskip`.
* The `skipper` package can load routes from both `eskip` config file and Kubernetes Ingress resource objects.
* The internal package `proxy` of `skipper` dynamically changes routing table populated by `skipper`.
* A route is one entry in the routing table, consisting of `predicates` for routing HTTP requests, `filters` for modifying the request and the response, and a backend, or `<shunt>` or `<loopback>`.

![Skipper Architecture](https://opensource.zalando.com/skipper/img/architecture.svg)

For each request,

* Proxy creates a context enhancing it with an Opentracing API Span.
* Proxy checks ratelimits and lookup the route in the routing table.
* Skipper applies request filters.
* Skipper checks route local ratelimits, the circuitbreakers and do the backend call.
  * Skipper automatically retry if there is TCP/TLS connection error.
* Skipper applies response filters.

Two special cases that skipper won't forward requests to the backend.

* Shunted route(`<shunt>`): skipper serves the request alone, by using only the filters.
* Loopback route(`<loopback>`): the request is passed to the routing table for finding another route, based on the changes that the filters made to the request.

![Request](https://opensource.zalando.com/skipper/img/req-and-resp-processing.svg)

## Tools for creating beautiful diagrams

[news.ycombinator.com](https://news.ycombinator.com/item?id=18788244)

Tools mentioned in the thread: yeD, draw.io, visio, plantuml, simplediagrams, AsciiFlow, Gliffy, Omnigraffle, Graphviz, ipe, Dia, gravit.io, limnu.com, LucidChart, plotdevice.io, monodraw, mermaid, whimsical.co, LaTeX/TikZ, ...

> This is HN at it’s best, nobody answered the original question.

## Understanding Parser Combinators

[fsharpforfunandprofit.com 1](https://fsharpforfunandprofit.com/posts/understanding-parser-combinators/) | [fsharpforfunandprofit.com 2](https://fsharpforfunandprofit.com/posts/understanding-parser-combinators-2/) | [fsharpforfunandprofit.com 3](https://fsharpforfunandprofit.com/posts/understanding-parser-combinators-3/) | [fsharpforfunandprofit.com 4](https://fsharpforfunandprofit.com/posts/understanding-parser-combinators-4/)

The series provides a step-by-step tutorial on creating a basic JSON parser library by parser combinators. Although the implementation is in F#, the concept is the same in other programming languages.

* Step 1: The parser gives the result (true, "remaining characters") or (false, "original characters"), according to if the parser can successfully consume the characters.
* Step 2: The parser gives the result ("success message", "remaining characters") or ("error message", "original characters").
* Step 3: The parser gives the result Success(charToMatch, remaining) or Failure(message).
* Step 4: Convert the parser function to a curried implementation to support more characters.
* Step 5: Encapsulate the parsing function in a type.
* Step 6: Implement andThen combinator. For example, `parserA andThen  parserB` = handle char A, and then handle char B.
* Step 7: Similarly, implement "orElse" combinator.
* Step 8: Use `mapP` to support parsing multiple characters at once and aggregating them into a string. For example, `run parseThreeDigitsAsInt "123A"` gives `Success(123, "A")`
* Step 9: Use `sequence` to transform a list of parsers into a single parser. For example, `let combined = sequence [parseThreeDigitsAsInt; parserA]` can parse "123AZ" to `Success([123, 'A'], "Z")`.
* Step 10: Use `many` / `many1` to match a parser multiple times, pretty much like regex `(pattern)*` and `(pattern)+`.
* Step 11: Throw some results away. For example, when parsing `'abc'`, we want the result `String("abc")`, not `String("'abc'")`.
* Step 12: Add error messages if the input is incorrect.
* Step 13: Apply all blocks introduced above and define the JSON parsing rules <https://www.json.org/>

## Many Nodes, One Distributed System

[medium.com](https://medium.com/baseds/many-nodes-one-distributed-system-9921f85205c4)

* A single process or computer that does not communicate with others is **NOT** a distributed system.
* A distributed system involves multiple entities talking to one another in some way, while also performing their own operations.
* If the system is a bunch of distributed servers, then maybe the components could be referred to as “servers”; if the system involves processes talking to one another, then perhaps the entities are just “processes”. Whatever it is, we call it nodes, the individual entities in a distributed system.
* The nodes in a distributed system run their own operations - these operations are fast. But the nodes also communicate with each other - communication is slow, which happens to be one of the biggest problems in distributed computing.

## The state of gRPC in the browser

[grpc.io](https://grpc.io/blog/state-of-grpc-web)

* The challenge of gRPC in the browser: There is no way to force using HTTP/2. Also, The browser doesn't support accessing raw HTTP/2 frames.
* How to rescue? Place a gRPC-Web as a proxy in between the browser side and the gRPC server side. The proxy can support both HTTP/1 and HTTP/2.
* There are two implementations of gPRC-web: [improbable-eng/grpc-web](https://github.com/improbable-eng/grpc-web) and [grpc/grpc-web](https://github.com/grpc/grpc-web).

![](https://grpc.io/img/grpc-web-proxy.png)

## How To Speed Up The Code Review

[sergeyzhuk.me](https://sergeyzhuk.me/2018/12/29/code_review/)

Review strategies:
* Feature changes: fulfillment of business requirements and design.
* Structure refactoring: backward compatibility and design improvements.
* Simple refactoring: readability improvements. Because these changes are mostly may be done by IDE.
* Renaming/removing classes: whether the namespaces structure has become better.
* Removing unused code: backward compatibility.
* Code style fixes: in most cases instant merge.
* Formatting fixes in most cases instant merge.
* Simple refactoring. The code has become more readable? - merge.
* Renaming/moving classes. The class has been moved to a better namespace? - merge.
* Removing unused (dead) code - merge.
* Code style or formatting fixes - merge, people shouldn’t review them, it is the task for linters.

**Don’t create huge pull-requests with mixed categories of changes.**

* Optimize your code for reading. The code is read much more often than it is written.
* Describe suggested changes in order to provide a context for diffs in the request.
* You should review your code by yourself before submitting the request. Review your own request as it is not yours. Sometimes you may find something you have missed. This will reduce the circles of rejecting/fixing.

## Monorepo: Please do v/s Please don't

[do](https://medium.com/@adamhjk/monorepo-please-do-3657e08a4b70) v/s [don't](https://medium.com/@mattklein123/monorepos-please-dont-e9a279be011b)

[Monorepo](https://en.wikipedia.org/wiki/Monorepo) a software development strategy where code for many projects are stored in the same repository.

* Though we get the benefit of easier collaboration / code sharing / single build / no dependency management, it creates tight coupling, and more importantly, at scale, a monorepo must solve every problem that a polyrepo must solve, with the downside of encouraging tight coupling, and the additional herculean effort of tackling VCS scalability.
* The default behavior of a polyrepo is isolation — that’s the whole point. The default behavior of a monorepo is shared responsibility and visibility — that’s the whole point.
